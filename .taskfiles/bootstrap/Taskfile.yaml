---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

vars:
  GITHUB_DEPLOY_KEY_FILE: '{{.KUBERNETES_DIR}}/bootstrap/flux/github-deploy-key.sops.yaml'

tasks:
  talos:
    desc: Bootstrap the Talos cluster
    dir: '{{.TALHELPER_DIR}}'
    cmds:
      - '{{if eq .TALHELPER_SECRET_EXISTS "false"}}talhelper gensecret > {{.TALHELPER_DIR}}/talsecret.sops.yaml{{end}}'
      - '{{if eq .TALHELPER_SECRET_EXISTS "false"}}sops --encrypt --in-place {{.TALHELPER_DIR}}/talsecret.sops.yaml{{end}}'
      - talhelper genconfig
      - cp {{.TALHELPER_DIR}}/clusterconfig/talosconfig $HOME/.talos/config
      - talhelper gencommand apply --extra-flags="--insecure" | bash
      - until talhelper gencommand bootstrap | bash; do sleep 10; done
      - until talhelper gencommand kubeconfig --extra-flags="{{.ROOT_DIR}} --force" | bash; do sleep 10; done
      - cp {{.ROOT_DIR}}/kubeconfig $HOME/.kube/config
    vars:
      TALHELPER_SECRET_EXISTS:
        sh: test -f {{.TALHELPER_DIR}}/talsecret.sops.yaml && echo true || echo false
    preconditions:
      - test -f {{.ROOT_DIR}}/.sops.yaml
      - test -f {{.SOPS_AGE_KEY_FILE}}
      - test -f {{.TALHELPER_DIR}}/talconfig.yaml
      - which talhelper sops
  nas:
    desc: Bootstrap NAS to join the cluster
    summary: |
      - ensure you do the configuration that's in kubernetes/bootstrap/talos/patches/global manually!
        especially the machine-network and machine-files ones!
      - ensure HAProxy is setup, or KubePrism will cause Cilium to fail:
        defaults
          timeout client 10s
          timeout connect 5s
          timeout server 10s

        frontend kubeprism
          mode tcp
          bind 127.0.0.1:7445
          default_backend k8s_api

        backend k8s_api
          mode tcp
          server lb 192.168.20.2:6443 check
          server c1 192.168.20.61:6443 check backup
          server c2 192.168.20.62:6443 check backup
          server c3 192.168.20.63:6443 check backup

    prompt: Wipe existing K8S install from NAS and re-initialize? Will also reboot the NAS!
    cmds:
      # remove existing
      - cmd: >
          {{.SSH}} 'systemctl is-active kubelet.service containerd.service 1>/dev/null && sudo systemctl disable kubelet.service containerd.service && sudo reboot && sleep 10'
        ignore_error: true
      - until {{.SSH}} -o ConnectTimeout=1 'exit 0'; do sleep 5; done
      - >
        {{.SSH}} 'sudo find /var/lib/containerd/io.containerd.snapshotter.v1.btrfs/snapshots/ -maxdepth 1 -type d -exec btrfs subvolume delete {} \; ; sudo rm -rf /etc/kubernetes /etc/cni /var/lib/cni /opt/cni /var/lib/containerd /var/lib/kubelet'
      # copy files from talos cluster
      - >
        {{.TALOS}} cat /etc/kubernetes/kubeconfig-kubelet > "$TMPDIR/kubelet.conf"
      - >
        {{.TALOS}} cat /etc/kubernetes/bootstrap-kubeconfig > "$TMPDIR/bootstrap-kubelet.conf"
      - >
        {{.TALOS}} cat /etc/kubernetes/pki/ca.crt > $TMPDIR/ca.crt
      - >
        sed -i "/server:/ s|:.*|: https://192.168.20.2:6443|g" "$TMPDIR/kubelet.conf" "$TMPDIR/bootstrap-kubelet.conf"
      - |
        cat > "$TMPDIR/var-lib-kubelet-config.yaml" <<EOT
        kind: KubeletConfiguration
        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
          anonymous:
            enabled: false
          webhook:
            enabled: true
          x509:
            clientCAFile: /etc/kubernetes/pki/ca.crt
        authorization:
          mode: Webhook
        clusterDomain: "$CLUSTER_DOMAIN"
        clusterDNS: $CLUSTER_DNS
        runtimeRequestTimeout: "0s"
        cgroupDriver: systemd
        containerRuntimeEndpoint: unix:/$SOCKET_PATH
        EOT
      - |
        cat > "$TMPDIR/kubelet.service.override.conf" <<EOT
        [Service]
        Environment="KUBELET_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --fail-swap-on=false"
        EOT
      - scp -r "$TMPDIR" "nas.servers.internal:$TMPDIR"
      - >
        {{.SSH}} "sudo mkdir -p /etc/kubernetes/pki /var/lib/kubelet /etc/systemd/system/kubelet.service.d/ &&
          sudo mv $TMPDIR/kubelet.conf /etc/kubernetes/kubelet.conf &&
          sudo mv $TMPDIR/bootstrap-kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf &&
          sudo mv $TMPDIR/ca.crt /etc/kubernetes/pki/ca.crt &&
          sudo mv $TMPDIR/var-lib-kubelet-config.yaml /var/lib/kubelet/config.yaml &&
          sudo mv $TMPDIR/kubelet.service.override.conf /etc/systemd/system/kubelet.service.d/override.conf &&
          sudo systemctl daemon-reload &&
          sudo systemctl enable --now containerd.service kubelet.service"
    vars:
      SSH: ssh -t nas.servers.internal
      TALOS: talosctl -n 192.168.20.2
    env:
      TMPDIR:
        sh: mktemp -d
      CLUSTER_DOMAIN:
        sh: talosctl -n "192.168.20.2" get kubeletconfig -o jsonpath="{.spec.clusterDomain}"
      CLUSTER_DNS:
        sh: talosctl -n "192.168.20.2" get kubeletconfig -o jsonpath="{.spec.clusterDNS}"
      SOCKET_PATH: /var/run/containerd/containerd.sock
    preconditions:
      - talosctl --nodes 192.168.20.2 get machineconfig
      - ssh nas.servers.internal -o ConnectTimeout=1 'exit 0'
      - which kubectl talosctl ssh

  apps:
    desc: Bootstrap apps into the Talos cluster
    summary: |
      IMPORTANT: All nodes will be used for OSDs and the rook drives will be wiped!
    prompt: Bootstrap apps into the Talos cluster?
    cmds:
      #- until kubectl wait --for=condition=Ready=False nodes --all --timeout=600s; do sleep 10; done
      - kubectl create namespace flux-system --dry-run=client -o yaml | kubectl apply --server-side --filename -
      - '{{if eq .GITHUB_DEPLOY_KEY_EXISTS "true"}}sops exec-file {{.GITHUB_DEPLOY_KEY_FILE}} "kubectl apply --server-side --filename {}"{{end}}'
      - '{{if eq .SOPS_SECRET_EXISTS "false"}}cat {{.SOPS_AGE_KEY_FILE}} | kubectl --namespace flux-system create secret generic sops-age --from-file=age.agekey=/dev/stdin{{end}}'
      - kubectl create namespace external-secrets --dry-run=client -o yaml | kubectl apply --server-side --filename -
      - '{{if eq .BITWARDEN_SECRET_EXISTS "false"}}cat {{.BITWARDEN_KEY_FILE}} | tr -d "\r\n " | kubectl --namespace external-secrets create secret generic bitwarden-secrets-manager --from-file=token=/dev/stdin{{end}}'
      - sops exec-file {{.KUBERNETES_DIR}}/flux/components/namespace/cluster-secrets.sops.yaml "kubectl apply --server-side --filename {}"
      - kubectl apply --server-side --filename {{.KUBERNETES_DIR}}/flux/components/namespace/cluster-settings.yaml
      - 'echo "node count: $NODE_COUNT"'
      - helmfile --file {{.KUBERNETES_DIR}}/bootstrap/helmfile.yaml apply --skip-diff-on-install --suppress-diff
    env:
      NODE_COUNT:
        sh: talosctl config info --output json | jq --raw-output '.nodes | length'
    vars:
      GITHUB_DEPLOY_KEY_EXISTS:
        sh: test -f {{.GITHUB_DEPLOY_KEY_FILE}} && echo true || echo false
      SOPS_SECRET_EXISTS:
        sh: kubectl --namespace flux-system get secret sops-age &>/dev/null && echo true || echo false
      BITWARDEN_SECRET_EXISTS:
        sh: kubectl --namespace external-secrets get secret bitwarden-secrets-manager &>/dev/null && echo true || echo false
    preconditions:
      - test -f {{.KUBECONFIG}}
      - test -f {{.KUBERNETES_DIR}}/bootstrap/helmfile.yaml
      - test -f {{.SOPS_AGE_KEY_FILE}}
      - test -f {{.KUBERNETES_DIR}}/bootstrap/talos/clusterconfig/talosconfig
      - test -f {{.KUBERNETES_DIR}}/bootstrap/templates/wipe-rook.yaml.gotmpl
      - test -f {{.KUBERNETES_DIR}}/flux/components/namespace/cluster-secrets.sops.yaml
      - test -f {{.KUBERNETES_DIR}}/flux/components/namespace/cluster-settings.yaml
      - which helmfile kubectl sops
